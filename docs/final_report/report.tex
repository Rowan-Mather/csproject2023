\documentclass[12pt, a4paper]{article}
\usepackage{setspace}
\usepackage{graphicx} % Required for inserting images
\usepackage[margin=1in]{geometry}
\usepackage[table,xcdraw, dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage[acronym]{glossaries}
\usepackage{float}
\usepackage{verbatim}
\usepackage{listings}
\usepackage[]{mdframed}
\usepackage{gensymb}
\usepackage[shortlabels]{enumitem}
\usepackage{multicol}
\usepackage{rotating}
\usepackage{longtable}
\usepackage{caption}

\definecolor{mygrey}{rgb}{0.3, 0.3, 0.3}
\newcommand{\light}[1]{\textcolor{mygrey}{#1}}

\definecolor{brycedoesntlikegrey}{rgb}{0.1, 0.5, 0.1}
\newcommand{\done}[1]{\textcolor{brycedoesntlikegrey}{#1}}

\newcommand{\must}[1]{\textcolor{red}{#1}}
\newcommand{\should}[1]{\textcolor{orange}{#1}}
\newcommand{\could}[1]{\textcolor{green}{#1}}

\bibliographystyle{ieeetr}

\makeglossaries

\newglossaryentry{VR}
{
        name=Virtual Reality (VR),
        description={“A computer-generated simulation of a lifelike environment that can be interacted with in a seemingly real or physical way” (Oxford English Dictionary \cite{oed:virtualreality}}
}  

\newglossaryentry{AR}
{
        name=Augmented Reality (AR),
        description={“The addition of computer generated output … to a person's view or experience of … physical surroundings by means of any of various electronic devices” (Computer Graphics Forum) \cite{3dmodel}}
}  

\newglossaryentry{3dmodel}
{
        name=3D Model,
        description={“A numerical description of an object that can be used to render images of the object” (Oxford English Dictionary) \cite{oed:augmentedreality}}
}  

\newglossaryentry{texture}
{
        name=Texture,
        description={A 2D image mapped onto a polygon or 3D model to add extra detail without more 3D points \cite{texture}}
}

\newglossaryentry{render}
{
        name=Render,
        description={The complete process of producing a 2D image from the virtual 3D world \cite{render}}
}  

\newglossaryentry{site}
{
        name=Site,
        description={A collection of 3D models tied to the same geographic location}
}  

\newglossaryentry{live}
{
        name=Manual/Live,
        description={Moving around the virtual world with arrow buttons / actively using the mobile device’s camera, GPS location, and orientation for this}
}  

\begin{document}

\setstretch{1.5}

\begin{titlepage}
\centering
    \centering
    \vspace*{1cm}

    \hrule
    \vspace*{1cm}  
    {\huge Reconstructing Historical Sites with Augmented Reality}
    \vspace*{1 cm}  
    \hrule
    
    \vspace*{2.2cm}  

    \begin{figure}[H]
    \centering
        \includegraphics[width=0.5\linewidth]{figures/archaelogodark.PNG}
    \end{figure}

    \vspace*{1.5cm}  
        {
    \Large
    April 2024
    }
    \vspace*{2.2cm} 
    
    \begin{minipage}{.48\textwidth}
        \large
        \raggedright
        \textit{Author} \par 
        Rowan Mather \par u2100495@live.warwick.ac.uk
    \end{minipage}
    \begin{minipage}{.02\textwidth}
    \hspace{1pt}
    \end{minipage}
    \begin{minipage}{.48\textwidth}
        \raggedleft
        \large
        \textit{Supervisor} \par 
        Dr Claire Rocks \par c.l.rocks@warwick.ac.uk
    \end{minipage} 
            
\end{titlepage}

\newpage

\vspace*{1cm}

\begin{center}
\subsubsection*{Abstract}
\end{center}
Augmented Reality as an educational tool is becoming increasingly popular, but there is limited work in enabling people to create their own experiences. This is especially the case within the museum and archaeological sectors, where only some bespoke applications are available. Over the last year, development on ARchae (short for archaeology, or the Augmented Reality Constructed Historical Architecture Environment) has been taking place. This mobile app built in the Unity Game Engine is designed to place inputted 3D models anywhere in the world for users of the app to walk around and explore. By positioning a virtual environment on top of the real one, curators can show visitors how their ruined historical site used to look and how it has changed over time.

\begin{center}
\textbf{Keywords:} Graphics, Unity, Augmented Reality, Education, Mobile, GPS
\end{center}

\newpage

\printglossary

\section*{Resources}
A reader may find it useful to refer directly to the below resources in synergy with the descriptions of this report.

Code: \url{https://github.com/Rowan-Mather/csproject2023/tree/archaev1.1} \cite{tools:repo} \\
Site Repository: \url{https://github.com/Rowan-Mather/csproject2023/tree/sites} \cite{tools:repo} \\
ARchae Demo Video: \url{https://www.youtube.com/watch?v=xtoHQ3a2LKU} \cite{design:videodemo} \\
Project Presentation Slides: \\
\url{https://docs.google.com/presentation/d/1n2Qd-FJX9IeHvG05MpUy42vssR4PQ2IRw_TUOAio5jk/}

\newpage

\tableofcontents

\newpage

\section{Introduction}

\subsection{Preface}
Overcoming the dissonance between a modern day landscape and its historical counterpart can be challenging if it has long gone to ruin. To aid visitors, many popular historical sites provide an immersive series of images showing historians' best guesses as to how it would have looked. Sketches on interpretation panels are commonplace (such as the examples at Bury St Edmunds Abbey in Figure \ref{fig:stedmundschurch} and the White Cliffs of Dover in Figure \ref{fig:iloveyoubryce}), but we are increasingly seeing additional \gls{VR} or \gls{AR} experiences. A VR set-up would involve an entirely computer-generated series of images (see Figure \ref{fig:VRvsAR}). Although not directly history-oriented, the VR gallery at the ArtScience Museum in Singapore is an excellent example of this: visitors are given a headset which allows them to access a virtual exhibition (see Figure \ref{fig:singaporeartsci}). In addition, the Isabella Stewart Museum in Boston makes use of AR, drawing together the real world and virtual one to complete its painting collection. The empty frames of stolen works are digitally filled in and are viewable on a mobile device (see Figure \ref{fig:bostonmuseum}). Similarly, the app RomeMVR is designed to show reconstructed historical points of interest in Rome overlaid on their real-world remains. Users can switch to the `time leap mode' and watch the 3D models slowly fading to reveal how the structures have changed (see Figure \ref{fig:romemv2intro}). 

\begin{figure}
\centering
    \includegraphics[width=0.95\linewidth]{figures/introduction/stendmunds.jpg}
    \caption{Interpretation panel at the ruins of The Abbey of Bury St Edmunds, displaying a digitally modelled image of the original structure \cite{preface:bury}.}
    \label{fig:stedmundschurch}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/introduction/doverdocks.png}
    \caption{An interpretation panel with a sketch of the 19th century docks in Dover \cite{preface:dover}.}
    \label{fig:iloveyoubryce}
\end{figure}

\begin{figure}[]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.926\linewidth]{figures/introduction/Guests-at-VR-Gallery-AspirantSG.jpg}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/introduction/Aquasia-VR-tile-1400x858.jpg}
\end{minipage}
\caption{The VR gallery at the ArtScience Museum in Singapore. The left image shows the set-up and the right an example exhibition: \textit{Aquasia and Geylang Crunk} \cite{preface:artscience}.}
\label{fig:singaporeartsci}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/introduction/bostonmuseum.jpg}
    \caption{The AR app used at the Isabella Stewart Museum in Boston to see stolen paintings as if they were still in their frames \cite{preface:bostommuseum}.}
    \label{fig:bostonmuseum}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/introduction/romemvr.PNG}
    \caption{A view of the Colosseum reconstruction in situ using RomeMVR \cite{existing:romemvr}.}
    \label{fig:romemv2intro}
\end{figure}

To aid the creation of such an augmented reality experience is the primary goal of this project. The system enables users to upload their own \gls{3dmodel}s to a web-server corresponding to certain time periods. A collection of these models make up a site. They can then add tags with extra written detail and finally view them overlaid in their real-world location. Archaeological researchers and curators of lesser-known historical sites can make the subject of their study accessible on a mobile device with the app: ARchae.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.47\linewidth]{figures/introduction/VRvsAR.jpg}
    \caption{Graphic showing the difference between VR, AR, and MR. \cite{originality:arvrmr}}
    \label{fig:VRvsAR}
\end{figure}

\newpage
\subsection{Project Selection}
Having formulated many different ideas for a third year project, this was initially rejected on the grounds that it would already exist. Upon further research, this turned out to be a completely false assumption. There are several applications of a somewhat similar nature, but such a generalist tool is yet to be published. This indicates at the utility of this project - it was genuinely surprising to find it was a gap in the market.

The area was chosen in mind of this author's interdisciplinary interests. With a background in 3D artistry it seemed appropriate to look at the computational aspects of handling models. In addition, creating different test cases in Blender was easier with some prior knowledge. Combining with the historical focus created a strong motivation to contribute something meaningful to a primarily non-technical industry.

\subsection{Augmented Reality in Education}
Studies show that the use of augmented reality in education, when combined with more typical learning styles (such as lecture form or tutoring), significantly benefits: \cite{education:ARlit}

\begin{multicols}{2}
    \begin{itemize}
    \item Motivation
    \item Attention
    \item Memory
    \item Accessibility
    \item Collaboration
    \item Satisfaction
    \end{itemize}
\end{multicols}

It is ideal for students that struggle with the written word \cite{education:class} - it is innately visually oriented. A well-designed experience may include some text but should predominantly take advantage of the independent, exploratory possibilities. Exact details may be provided through a separate medium in the classroom, but a student should be able to grasp the basic concepts and ideas of the material with no trouble if they can discover it for themselves. In addition, AR can be an excellent substitute for inaccessible locations or materials \cite{education:placespotentials}. This could be virtually visiting museums or points-of-interest without significant monetary investment, or running simulations of experiments that would otherwise be deemed too unsafe or complex.

However, when attempting to utilise a system with over-complicated interfaces without proper teacher training, the opposite effect is inevitable. Absorbing many new digital skills on top of the main teaching material may lead to students being “cognitively overwhelmed” \cite{education:overwhelm}. It is therefore crucial to design simple AR systems that progressively walk a user through more functionality as needed so that they do not immediately reject the technology. 

There is clearly a keenness to try out such systems, but a lack of understanding. A goal of this project is to progress the inclusion of such AR tools in general education, by making set-up and customisation as widely accessible as possible. 

\subsection{Initial Interviews}
To refine the scope, several professionals with historical backgrounds were sought out. Each gave a different perspective on the uses and methods for implementing ARchae, which was ideal when determining the objectives and their prioritisation.

Firstly, within the University of Warwick's computer science department, Dr Rosella Suma was interviewed. Dr Suma is an expert in image processing and VR, but also has a background in virtual reconstruction of small historical artefacts. Her research interests therefore aligned well with that of this project, and her technical insights were invaluable for approaching the initial research. She recommended looking at photogrammetric techniques for model construction \cite{interviews:rosellapaper}. Photogrammetry is ``the science of obtaining reliable information about the
properties of surfaces and objects without physical contact with the objects'' \cite{interviews:photogrammetry}, involving image capture and often electromagnetic radiant energy. Since this is an already well-studied field, this project was focused on visualisation rather than generation. There are many available tools for generating of 3D models using photogrammetry, any of which would be suitable for a curator to use for input to ARchae.

Seeking guidance from an intended user of the software, an archaeologist who worked for the University of Cambridge was approached. Her opinion of the proposal validated it, saying ``this is just the kind of thing we would have wanted to have''. In industry it is typical to visualise smaller table-top items, in particular marine archaeologists will often take scans of their findings instead of collecting them for reasons of natural conservation and difficulty of bringing lodged items up from the sea bed. But she emphasised the importance of presenting the larger picture. For a general audience, contextualising the detailed objects is equally important. 

Finally, with the intention of ARchae to be used as an educational tool, the current Head of History from The Netherhall School in Cambridge was also interviewed. He described various extension features that would tailor it well to history education such as the the tags and timelines (see Objectives, Section \ref{Objectives}). His comments highlighted that a user should not simply see the change from some point in the past to the present, but be shown the story of how a site has continuously changed until it eventually becomes what they can see in front of them.

\subsection{Existing Work}
\label{existingwork}
Within academia, there have been an array of projects for visualisation of particular sites. The conference paper ``A Review on Augmented Reality for Virtual Heritage System" \cite{existing:virtualhertitagereview} neatly details several including a virtual Hagia Sophia \cite{existing:hagia} (Turkish Mosque, see Figure \ref{fig:hagiasophia}), Pompeii \cite{existing:pompeii} (Ancient Italian city), and Campeche 
\cite{existing:campeche} (Mexican city, see Figure \ref{fig:campeche}). The latter of which has been optimised with certain techniques such as Levels of Detail which allows a site to render at different qualities as needed and is detailed further in Section \ref{futurework}. Whilst there is some modification possible within their systems for non-programmers, the creation of such an experience is highly technical. Each case is an experiment in the capabilities of the technology using a site as the case study, with the exception of the virtual Pompeii which is more educationally-oriented. Therefore, the concept of digitally recreating the past is nothing new, but so far left predominantly in the hands of experts. The goal of this project to simplify the pipeline of creating such an experience, not removing the technical component of modelling but entirely removing the programming component.

\begin{figure}[]
\begin{minipage}{.48\textwidth}
  \centering
  \includegraphics[width=0.96\linewidth]{figures/introduction/campeche.PNG}
  \caption{Calle [road] 63 in Campeche as seen photographically and virtually \cite{existing:campeche}.}
  \label{fig:campeche}
\end{minipage}
\begin{minipage}{.04\textwidth}
  \hspace{0.1cm}
\end{minipage}
\begin{minipage}{.48\textwidth}
  \centering
  \includegraphics[width=0.94\linewidth]{figures/introduction/virtual hagia sophia.png}
  \caption{Two views of the 3D model \\ used for the virtual Hagia Sophia \cite{existing:hagia}.}
  \label{fig:hagiasophia}
\end{minipage}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{figures/introduction/timelooper.jpg}
    \caption{A TimeLooper installation at a museum in Gaffney, South Carolina with interactive figures from the American Revolution \cite{existing:timelooper}.}
    \label{fig:timelooper}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{figures/introduction/historicvr.png}
    \caption{A HistoricVR guided tour around Guildford Castle in VR \cite{existing:historicvr}.}
    \label{fig:historicvr}
\end{figure}

Considering tools which are more commercialised than research oriented, the details are even less well documented and lack customisation. Most existing systems are only on the edge of what could be considered VR, such as Timelooper \cite{existing:timelooper} (see Figure \ref{fig:timelooper}) which predominantly uses large LCD screens. Historic VR \cite{existing:historicvr} (see Figure \ref{fig:historicvr}) takes it a step further with their Guildford Castle set-up, allowing the user to interact with the virtual tour using a VR headset. These are good educational tools in their own right but maintain a disconnect between visitor and experience. By using AR, exploration of a virtual site is far more immersive and tangible. 

There are some sites that do this such as Tamworth Castle \cite{existing:tamworth} or the Colosseum in Rome MVR \cite{existing:romemvr} \cite{existing:historik} (see Figure \ref{fig:romemv2intro}). The uptake and comprehensibility of such applications is variable and they are rarely updated, but typically they are good representations of the structure and have a novelty that makes them appealing. The problem is that each application is built from scratch for that site so it is difficult to extend the scope without continuous input from the initial developers. Instead, consider international projects such as Google Earth \cite{existing:googleearth}, which has a feature to display a 3D model in the photographed world. This is more customisable but since the Google Earth views are pre-captured image data, it is difficult to work with this in an augmented reality sense, or to share creations with other users.

Whilst the primary use case for this project is in archaeological research and education, it is not limited to this. Looking at existing applications with similar features in a different context such as construction, tools such as Morpholio Sketchwalk \cite{existing:morpholio} (see Figure \ref{fig:morpholio}) and Arki \cite{existing:arki} (see Figure \ref{fig:arki}) are available. These are already successful in mitigating construction errors by showing workers precisely what the architect intended, complete with measurements and notes. In addition, they enable simple collaboration in design \cite{existing:ardesign} by showing a client what they can expect in situ before a brick is laid, and even allowing them to make instant modifications. Indeed the functionality of the construction software is in many ways closer to that of this project, but it needs re-contextualisation. The design of such applications are oriented around architecture, so a user would be able to change the model they are viewing, which does not make sense in a tour-guiding situation.

A summary of these applications and the useful features included in this project is given in Figure \ref{fig:existingwork}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/introduction/arki.png}
    \caption{A bridge being designed using Arki for construction \cite{existing:arki}.}
    \label{fig:arki}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/introduction/morpholio.png}
    \caption{A house being designed using Morpholio Sketchwalk for construction \cite{existing:morpholio}.}
    \label{fig:morpholio}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=650pt, angle=90]{figures/introduction/Existing work.pdf}
    \caption{A summary of the contexts and features of the existing work detailed in this report, where the green highlighting shows aspects in common with ARchae.}
    \label{fig:existingwork}
\end{figure}

\subsection{Originality}
This project in effect combines many of the most useful aspects of all the existing work into a single immersive application. It has three crucial distinguishing features which separate it from any single application made yet:
\begin{enumerate}
    \item \textbf{Augmented reality.} \\
    Visualising locations is mostly done on a standard monitor or in virtual reality (see Figure \ref{fig:VRvsAR}). For the most part this is sufficient, but to truly grasp what life was like in the past, it is best to place the structures in situ. As discussed in the interviews, context is crucial, especially if the user is unfamiliar with the terrain.
    \item \textbf{General-purpose.} \\
    Most augmented reality applications are bespoke, made by an external company to suit the exact needs and layout of the site. This project is not as tailored, but usable anywhere in the world for any model. Often smaller locations/projects will not have the resources to develop their own system, but can use ARchae freely instead.
    \item \textbf{GPS location-based.} \\
    Augmented reality applications that require the virtual object to be placed in a specific position often use some form of image `anchor' or `marker', such as a table (see Figure \ref{fig:ARmarker}). As a user moves their phone, the app tracks the relative position of the table and keeps the virtual object still. This is an effective method of exact placing, but would require a permanent installation at the site. They are also best suited to applications where the user does not move too far from the anchor and keeps it permanently in their field-of-view. Since this project operates on a much larger scale, markers are not as suitable. A little precision is sacrificed in favour of mobility - instead the geographic position is used. This means the only hardware required for a user is a GPS (which are built into most mobile devices) and there are no required physical objects at the site for the curator to maintain. 
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/introduction/example-of-marker-based-AR.jpg}
    \caption{An example of a typical AR marker (which this project will be not be using). \cite{originality:armarker}}
    \label{fig:ARmarker}
\end{figure}

\subsection{Objectives}
\label{Objectives}
Set out in the specification, the objectives have remained unchanged throughout the project. These define exactly what the initial technical goals were for implementation, broken down by priority: 
\begin{itemize}
    \item Must-Have \must{(M)}
    \item Should-Have \should{(S)}
    \item Could-Have \could{(C)}
\end{itemize}

They are also broken down by area. The three main overarching categories of work set out in the methodology correspond as such:
\begin{itemize}
    \item \ref{modelimporting} `Model Importing': Objectives I and II 
    \item \ref{movement} `Movement': Objective III 
    \item \ref{augmentedrealitysynthesis} `Augmented Reality Synthesis': Objective III (final points) and Objective IV 
\end{itemize}

The priority was to have a Minimum Viable Product first, as further detailed in Section \ref{methodology}. This is the Must-Haves, and for the most part made up of Objectives I-III.

\newpage
\begin{enumerate}[label=\Roman*.]
\item \textbf{User may upload a model to a remote server.}
    \begin{enumerate}[label=\arabic*.]
    \item \must{(M)} Models will be in the wavefront (.obj) form.
    \item \could{(C)} Models can be uploaded in additional common formats e.g. .fbx, .dae, .blend
    \item \must{(M)} User may upload a texture (See Glossary: \gls{texture}) and other metadata to pair with the model.
    \item \should{(S)} A subset of the server models can be externally selected to be available in app.
    \item \could{(C)} Users cannot upload without submitting a review request.
    \end{enumerate}
    
\item \textbf{User may download models from the server into their local app.}
    \begin{enumerate}[label=\arabic*.]
    \item \must{(M)} Models can be imported from the server in their pure object form.
    \item \should{(S)} They can be coloured/textured according to an accompanying texture file (.mtl).
    \item \should{(S)} Users can select a subset of the server models to import locally.
    \item \should{(S)} Models will render (See Glossary: \gls{render}) with their textures and metadata.
    \item \must{(M)} Models will render with reasonable latency, generally a maximum of 2 seconds per model.
    \end{enumerate}

\item \textbf{User will be able to set their location and move around the models.}
    \begin{enumerate}[label=\arabic*.]
    \item \must{(M)} User has a manually configurable geo-location (latitude \& longitude).
    \item \must{(M)} User has a manually configurable orientation.
    \item \must{(M)} Models can be placed in a geo-location relative to the user.
    \item \should{(S)} Models will render only when within a certain distance to the user.
    \item \should{(S)} User can change the render distance.
    \item \could{(C)} User can change the scaling of the whole scene.
    \item \should{(S)} User location can be set to their actual mobile device's geo-location.
    \item \should{(S)} User can move freely while the app tracks their geo-location.
    \item \should{(S)} Compass direction can be set to that of device.
    \item \should{(S)} Tilt can be set to the spirit level of device.
    \item \should{(S)} User can switch between manual and device modes.
    \item \should{(S)} Add camera input as background for the scene.
    \item \could{(C)} Have a slider for model transparency.
    \end{enumerate}
    
\item \textbf{Additional information about the sites will be accessible.}
    \begin{enumerate}[label=\arabic*.]
    \item \could{(C)} Models can be tagged with information in a specific relative location to the model.
    \item \could{(C)} Tags can be clicked to view further information/photos.
    \item \could{(C)} Tags can be shown/hidden in scene.
    \item \could{(C)} A date (range) will be in the corner corresponding to the age of the site you are viewing.
    \item \could{(C)} A timeline scroll will be at the bottom of the screen.
    \item \could{(C)} Models can be attached to a specific time period and will only be visible when this is selected.
    \end{enumerate}

\item \textbf{The app will be intuitive and guided.}
    \begin{enumerate}[label=\arabic*.]
    \item \could{(C)} Include a menu with an app tutorial.
    \item \could{(C)} Most information can be hidden so the user is not overloaded.
    \item \should{(S)} The default view will be a simple drop-down list of available models and device mode.
    \end{enumerate}
\end{enumerate}

\newpage
\section{Project Management}

\subsection{Methodology}
\label{methodology}
As evident from the step-by-step structure of the Gantt chart in Figure \ref{fig:gantt}, the methodology for this project is similar to the standard Waterfall Method. Specifically, an agile version of the \textit{Incremental Method} \cite{management:incremental} was selected. This followed naturally from the decision to develop an Minimum Viable Product first, and then iterate, adding features as time allowed. 

The incremental method sets out development in distinct stages, or increments:
\begin{enumerate}
    \item Requirements analysis
    \item Design
    \item Code 
    \item Test
\end{enumerate}
In the standard model the requirements are fixed for the duration of the increment, and only one increment is considered at a time in development. This makes it ideal for creating a basic but complete version of the product quickly and for ensuring robustness from the ongoing testing. 

In the case of this project, a slight adaptation had to be made since a plan spanning the whole development period was required. In addition, some change to the requirements in an increment as it was being developed would be permitted where the exact techniques were unknown, but this was kept to a minimum.  

\subsubsection{Increment 1 Specification}
The aim was to have a \textit{Minimum Viable Product} by the Christmas holiday (the Must-Haves in Section \ref{Objectives} Objectives). This involved:

\begin{itemize}
    \item Completing all initial research necessary research to set up the Unity project.
    \item Import models from a server.
    \item Display the models in the virtual world.
    \item Move around the virtual world manually. 
\end{itemize} 

\subsubsection{Increment 2 Specification}
Having completed Increment 1 early, most of Increment 2 was intended for the end of term and the Christmas holiday. The aim was to have a fully functional \textit{virtual reality} app: 
\begin{itemize}
    \item Build the app onto a mobile device.
    \item Get the device's GPS location.
    \item Get the device's orientation. 
    \item Correspond these measurements to the virtual world.
    \item Switch between the live and manual movement.
\end{itemize} 

\subsubsection{Increment 3 Specification}
\label{increment3}
Finally, Increment 3 was completed during Term 2 and involved converting the app to work with \textit{augmented reality}, and adding as many extension features as possible. There were two directions that this project could take as extension: either focusing on the UI, or adding optimisations. It was decided that since the intention was accessibility and the system was coping well with load in testing so far, the priority should be to have a clean layout:
\begin{itemize}
    \item Add the camera input as the background.
    \item Add additional labelling information (tags).
    \item Add a timeline to switch between periods. 
    \item Add a main menu.
    \item Add a side menu for selecting a subset of models.
    \item Add a tutorial.
\end{itemize} 

\newpage

\begin{figure}[H]
    \includegraphics[width=640pt, angle=90]{figures/projectmanagement/ganttendt3.pdf}
        \caption{Gantt Chart showing progress by the end of Term 3. \\ \light{Grey}: scheduled work. \textcolor{ForestGreen}{Dark green}: work on schedule. \textcolor{YellowGreen}{Pale green}: work off schedule.}
        \label{fig:gantt}
\end{figure}

\subsection{Tracking Progress}
Each work session and its timings was noted in a Google Doc. This has been helpful in tracking bugs, returning to different focuses, and remembering what areas were the most time consuming.

More formally, tasks were laid out in the custom Gantt Chart in Figure \ref{fig:gantt}. This was created for the specification in week 2, then updated after Christmas with additional details for the specifics of the AR synthesis for Term 2. 

The planning was closely adhered to, especially in Term 1 since that was dedicated to building the core functionality. There was a little more freedom for experimentation in Term 2, since work was towards additional features. In particular, the tags and timeline features were worked on earlier due to the date of Evaluation Day (the opportunity for presenting to peers detailed in Section \ref{Evaluation}), which was not made available until after the term had been planned.

\subsection{Meetings}
Weekly meetings during term time were held with Dr Claire Rocks. These were useful for selecting the direction of this project at each increment, adhering to the plans and discussing the general organisation and marking of the third year project. Having ideas to present at meetings has been strongly motivating every week, and Claire's advice has been invaluable.

\newpage
\section{Design}
There are two closely interlinked sides to the development of this project: the design and implementation. This section deals with the tools used, the UI and top-down view of the implementation. Section \ref{implementation}: Implementation will then discuss the exact details of the code. This reflects the structure of the main development tool: the Unity game engine, which has a partially drag-and-drop editor. Each object in this editor is a component which normally has an associated C\# script to define its specific behaviour. There are a number of primitive components such as images or buttons which can exist without a script if little change is required from their default form. Similarly, scripts can also exist independently in order to define behaviour over the whole project.

\subsection{Tools}
\subsubsection{Unity}
Unity is one of the most popular game engines currently on the market \cite{design:unity} so it was a natural choice to use as the framework for this project. 

Platforms designed specifically for mobile app development such as Android Studio \cite{design:android} were also considered, however due to the graphical nature of this project, a game engine which is built for handling 3D models is ideal. The other leading engine on the market is Unreal \cite{design:unreal}, and this would have been a viable alternative since Unreal is known for its high quality graphical processing. Unity was primarily chosen over Unreal as it is generally considered more accessible for any skill level \cite{design:unrealvsunity}. Once over the initial learning curve, the interface is straightforward to interact with, and this author had basic experience with it. In addition, the support for augmented reality development in Unity is stronger - external companies that focus on AR development tend to make their tool-kits available in Unity first. It was expected that the AR component would be one of the greatest challenges of this project so this was an important consideration.

A summary of some of the strengths and weakness of this engine (independent of the other possible platforms) is given below.

\newpage
\textbf{Advantages of Unity}:
\begin{itemize}
    \item Designed for mobile game development.
    \item Designed for handling heavy graphics.
    \item Can interpret .blend, .obj, and other 3D model files implicitly.
    \item Has built-in support for certain mobile sensors.
    \item Has a quick run feature for mobile so the project does not have to be fully built for each test. 
    \item Has a vast array of libraries, assets and contributors.
\end{itemize}

\textbf{Disadvantages of Unity}:
\begin{itemize}
    \item Steep learning curve for understanding the interface.
    \item Cannot work easily with files outside its project scope (see Section \ref{initiallocalattempt}).
    \item Requires large amounts of memory to store and run a project.
\end{itemize}

\subsubsection{Git \& GitHub}
\label{gitandgithub}
As a widely used tool, Git was selected for version control of the source code. 

In addition, the GitHub repository serves as the web-server for storing the 3D models. This was the simplest method for creating an easily modifiable online storage location for the files, and since web-dev was not the focus of this project, it more than sufficed. However, in a full release of the app, it would be wise to create a custom website to upload files to, and in the code it is elementary to change the location to download from.

The web-server is available here: \url{https://github.com/Rowan-Mather/csproject2023/tree/sites} \cite{tools:repo}

\subsubsection{Blender}
Blender \cite{design:blender} is a widely used, open-source 3D modelling tool. Although not directly used for a sub-system in this project, the software was employed heavily in testing to create 3D models to input to the ARchae system. Any common 3D software could work to this effect, but Unity has specific compatibility to work with the default output \verb|.blend| files which simplified early testing before the \verb|.obj| type was selected as the standard for the app.

\subsubsection{Devices}
The primary device for testing the system on was an Android Samsung Galaxy Tab S10, but to ensure general compatibility, the app was also installed on other phone mobile devices. 

\subsection{System Overview}
The extended ARchae system is not simply comprised of the mobile app, as shown in Figure \ref{fig:systemoverview}. It pulls in the uploaded 3D models from the GitHub server. The app also takes input directly from the mobile device, namely the camera for AR purposes, the gyroscope for determining rotation, and the GPS co-ordinates for location.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/design/contextmodel2.png}
        \caption{Context model-like diagram showing the interactions of the ARchae system and how the external systems feed into the three main software aspects: model importing, movement and augmented reality synthesis.}
        \label{fig:systemoverview}
\end{figure}


\newpage
\subsection{UI Construction}
\label{uidesign}
In Increment 3 (see \ref{increment3}), involving most of the UI design, the mock-up shown in Figure \ref{fig:uimockup} was drawn up. This final design is shown in Figures \ref{fig:menuandhelp} and \ref{fig:manualview} on a mobile in portrait instead of landscape, but most of the features and relative positioning remain the same. Throughout this report there may be some variation in the presentation of the UI. Some adjustments were made at the end of term 3 to the cosmetics whilst testing of the functionality was in progress, but the final design is the one shown here. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/design/labelleduidesign.png}
        \caption{Mock-up of the application user interface with functionality labels.}
        \label{fig:uimockup}
\end{figure}

There are three main divisions of the interface the user can interact with. The first is the main menu and help screen shown in Figure \ref{fig:menuandhelp}. The second is the manual mode shown in Figure \ref{fig:manualview} where they navigate the world using the arrow buttons and touch control. Finally, the live mode shown in also shown Figure \ref{fig:manualview} is where the virtual world is navigated in alignment with the user's real location and device orientation.

All of the UI elements are made with Unity components. Figure \ref{fig:bigboi} shows the mapping of the components to the different development categories (colour-coded identically to the context-model in Figure \ref{fig:systemoverview}), and specifically the C\# scripts they are associated with.

An explanation of the configuration of each of the UI components is now given, and a detailed description of this associated code will be covered in Section \ref{implementation}: Implementation.

\newpage
\subsubsection*{ \centering The Final UI}
\vspace{1cm}
\begin{figure}[H]
\begin{minipage}{.5\textwidth}
  \raggedleft
  \includegraphics[width=0.65\linewidth]{figures/design/mainmenu.jpg}
\end{minipage}
\begin{minipage}{.5\textwidth}
  \raggedright
  \includegraphics[width=0.65\linewidth]{figures/design/helpscreen.jpg}
\end{minipage}
\caption{The main menu and help screen.}
\label{fig:menuandhelp}
\end{figure}

\begin{figure}[H]
    \begin{minipage}{.325\textwidth}
      \includegraphics[width=1\linewidth]{figures/design/updatedmanualdesign.jpg}
    \end{minipage}
    \begin{minipage}{.325\textwidth}
      \includegraphics[width=1\linewidth]{figures/design/updatedmanualsidebar.jpg}
    \end{minipage}
    \begin{minipage}{.325\textwidth}
      \includegraphics[width=1\linewidth]{figures/design/updatedlivedesign.jpg}
    \end{minipage}
  \caption{Views of the activated app. The left and centre images both show it in manual mode with the centre view having the side-bar open. The right image shows it in live mode.}
  \label{fig:manualview}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{figures/implementation/the big boi diagram.png}
        \caption{The complete list of all custom classes in ARchae categorised by area of development. The UI components are then numerically labelled with the class that defines their behaviour beyond the default Unity functionality.}
        \label{fig:bigboi}
\end{figure}

\newpage
\subsubsection{Live Button}
The live button is a toggle component containing the textures (images) for the active and inactive state. These switch automatically when pressed. In addition, pressing the button called the \verb|toggleLive| function in the \verb|MovementButtonsScript| which activates or deactivates the manual controls.

\subsubsection{Arrow Buttons}
The arrow buttons each have a bounding box (as shown in Figure \ref{fig:arrowkeysbounding}) on top of the actual arrow image. Initially, these boxes were simple button components, but this moves the user forward a fixed amount per press rather than for as long as they hold it. Instead, they now each have two event triggers, one for being pressed and the other for release. These could call functions directly but this leads to differences in virtual travel rate depending on the speed of the device's processor. In actuality, they are tied to boolean variables which are checked by the \verb|Update| function, which is called 30 times a second.

\subsubsection{Drag to Move}
The main modification from the sketch to the final design is the removal of the scroll wheel. It was implemented and the component images are shown in Figure \ref{fig:scroll}, but it was later replaced by dragging the screen to rotate due to the cumbersome nature of trying to move forward and turn simultaneously, and the dragging being a more common action.

Now, every frame the \verb|handleTouch| or \verb|handleMouse| function is called (Section \ref{manualmovement}), which pans the camera.

\begin{figure}[h]
\begin{minipage}{.48\textwidth}
  \centering
  \includegraphics[width=0.62\linewidth]{figures/design/arrowboundingbox2.PNG}
  \caption{The final arrow key design showing the bounding boxes of the button components for each direction.}
  \label{fig:arrowkeysbounding}
\end{minipage}
\begin{minipage}{.04\textwidth}
  \hspace{0.1cm}
\end{minipage}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.62\linewidth]{figures/design/combinedscrollimage.PNG}
  \caption{The sprite (interactive image) used for the initial implementation of the rotation scroll wheel.}
  \label{fig:scroll}
\end{minipage}
\end{figure}

\subsubsection{Timeline}
The timeline was built from a slider component, and the position of the handle is queried every frame to get date selection information. Each date label is cloned from a prefab (template). The prefab itself is based on a TextMeshPro object, which is a component that behaves like a 3D model but shows text. This means it automatically performs depth rendering calculations - that is even when placed with the other UI components, the labels will always be positioned correctly relative to site models in the scene, rather permanently at the front.

\subsubsection{Menu and Help Buttons}
The main menu and help screen were created using Unity panel components containing buttons. These call functions in the \verb|MainMenuScript| to hide and show the various components. The `?' button in the main application interface will return the user to this menu so they can always re-read the instructions.

\subsubsection{Sidebar}
Since there is no Unity component for menus such as this, a modified version of the external asset Simple Side Menu was used \cite{design:sidemenu}. This was chosen for its level of customisation - it was easy to edit the component to match the aesthetic of the app and many variables, such as the speed it slides at, could be configured. Moreover the asset provides a number of functions called when there is a change to the state of the side bar, which was used to toggle the image on the pull-out tab.

\subsubsection{Site List}
Also attached to the side bar is the list of all available sites. This is updated (by the \verb|SiteHolder|) every time a site is added or removed, typically in importing when first running the app. Enclosed in a vertical scroll bar component, many sites can be added and viewed in the list without it running out of space.

\subsubsection{GPS \& Year View}
These two components show the current location of the user and the year they are seeing virtually. Design-wise they are simply text attached to the side bar that can be updated by the \verb|LocationDisplayScript|. 

\subsubsection{Tags}
The tags are the labels which would normally be used to specify the different areas of a site e.g. `Gatehouse', `Spire', `Pulpit' etc. Similarly to the date labels on the time, the tags are visually created by instantiating from a prefab (template). This prefab has a \verb|TextMesh| - that is a 3D model which appears as text. It is necessary to use this model form since if a standard text component is used, the apparent position relative to the rest of the site will change depending on the perspective. The depth perception is determined by the shader (file defining how position, texture and colour is viewed) and the default shaders on UI text objects bring everything to the foreground.

\subsection{Neilson's Design Principles}
Where possible the interface has been designed in line with the well-known heuristics: Neilson's Design Principles \cite{design:neilsons}. This was to guide composition towards being complete but minimalist, so as to not be overwhelming to a new user.

\begin{enumerate}
    \item \textit{Visibility of system status} \\
    The system informs the user of their location, and the live toggle changes colour depending on the mode.
    
    \item \textit{Match between system and the real world} \\
    There isn't a great deal of technical terminology in the interface, so as long as the user understands the premise of the app, they should be confident with it. At present, the only language available is English, but this could be easily extended in a full release of the app. 
    \item \textit{User control and freedom} \\
    Every action taken is immediately undo-able with no lasting effect.
    \item \textit{Consistency and standards} \\
    The language is minimal, but consistent throughout. The GPS co-ordinates are converted into the standard, degree form.
    \item \textit{Error prevention} \\
    If data is entered incorrectly on the server side, it will not be pulled into the app. Otherwise there is little room for error which cannot be corrected straight away.
    \item \textit{Recognition rather than recall} \\
    The arrow buttons and timeline are well known symbols, and the left arrow is also a common indicator of an extensible side bar. Similarly, the question mark is an almost universal help symbol, and the two circles often indicate recording so is associated with a camera feed.
    \item \textit{Flexibility and efficiency of use} \\
    The side-bar never needs to be accessed for basic functionality of the app, neither does the timeline if only one model is available. The app always defaults to manual mode (with just the arrow button) so is at first a simple open-world style explorer.
    \item \textit{Aesthetic and minimalist design} \\
    The UI is fairly simple, and all additional information not intrinsic to the currently viewed model is hidden in the sidebar.
    \item \textit{Help users recognize, diagnose, and recover from errors} \\
    There is little error diagnosis at present since all error messages should be dealt with without user interaction.
    \item \textit{Help and documentation} \\
    The help button on the main menu details how to begin using the app and what it does, and the main menu is always accessible from the `?' button. For more detailed information, there is the tutorial/video demo online. \cite{design:videodemo}
\end{enumerate}

\newpage
\section{Implementation}
\label{implementation}
The implementation will cover the associated C\# code attached to the Unity project. There is also some auto-generated content present in the files defining the default behaviours of Unity itself and some of the external assets (libraries), but for the most part this will not be covered as it does not contribute to the logical workings of the solution.

For clarity, the three development areas are defined as such:
\begin{itemize}
    \item Model Importing: fetching the 3D models for each site from the web-server and displaying them in the app along with their additional tags and date information.
    \item Movement: shifting and looking around the virtual world using either the manual controls or mirroring the real-world location and rotation data from the user's device. 
    \item Augmented Reality Synthesis: adding the real camera input so that the virtual world appears to be overlaid.
\end{itemize}
By no means do these exist independently of each other, but the basic functionality for each were implemented in the above order.

Unity has a \verb|MonoBehaviour| base class that most scripts (which each define a class) will inherit from. This gives the script access to the MonoBehaviour methods, most importantly \verb|Start()| and \verb|Update()|. Start is called when the app first runs and Update is called repeatedly, around 30 times a second. In a typical game this might be used for animation - refreshing the image at a high frame rate, but in this it is helpful for collecting all the live data and updating the models many times a second. 

\subsection{Model Importing}
\label{modelimporting}

\subsubsection{Local Experimentation}
\label{initiallocalattempt}
Before introducing the web-server, it was attempted to import models more directly from the local memory. With code being written in C\#, all the standard runtime read/write functionality is available such as StreamReader \cite{models:streamreader}. As expected, within the scope of the Unity project (the collection of files that the Unity Editor interacts with), it is possible to read and write text files.

However, if one tries to use one of these standard read/write tools within a Unity project to access a file outside of the project scope, an error is thrown. Since an external access such as this cannot be built neatly into a self-contained application, it is highly unusual to attempt in a game engine.

The \verb|SimpleFileBrowser| is an asset which uses the Storage Access Framework on an Android device. This enables access to any permissible files at runtime. This successfully manages to import files, but the models will not display in the virtual world. Experimentation with this asset revealed that Unity performs a series of pre-processing functions when a model is imported normally at compile time. By doing it at runtime, this is by-passed. Another asset that simplifies the whole process was selected instead - the \verb|Runtime OBJ Importer| \cite{models:objimporter}.

\subsubsection{Using OBJ Files}
The Runtime OBJ Importer assists with downloading .obj files into the project from either the local memory or the web. It also applies this aforementioned pre-processing so is ideal. 

As suggested by the name the asset only works with object/wavefront (.obj) files without modification. This is one of the oldest and most widely used type of 3D model file (not to be confused with the machine code or bytecode term). Most pieces of 3D modelling software will export to it, so even with the restriction on only accessing object files, the system remains sufficiently general.

Inherently, the object files only describe the vertices of a model, so to add a colour/texture they must be accompanied by a material file (.mtl). Blender and other such modelling software will normally export the two file types together automatically, and the Runtime OBJ Importer also has some support for adding material files so adding this feature required little modification.

After experimenting with the importer asset locally, it was tested on some online repositories of 3D models such as the samples from Florida State University \cite{models:samplemodels} shown in Figure \ref{fig:basicico}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/implementation/importing/basicico.jpg}
        \caption{Initial experimentation with object importing on a icosahedron.}
        \label{fig:basicico}
\end{figure}

\subsubsection{Historical Site Server}
The GitHub web-server is the central repository for all the historical site information \cite{tools:repo}. Each site is organised into its own folder, containing its 3D models as object and material files along with the special \verb|metadata.txt| file. This file lists the site's location, date, and other such information not intrinsic to the models (see Figure \ref{fig:metadataexample}). Also on the server is a master file that contains all the names of the folders to be considered. When a site is first uploaded, its data is not immediately pulled into the app. First, you have to specify that it should be included by adding the name to the master file. 

\begin{figure}[h]
    \centering
\begin{mdframed}[leftmargin=50pt, rightmargin=50pt]
    \begin{verbatim}
location:52.379015480, -1.560941225, 140;
model_name: greek-theatre | date:340BC      |
    tag: Theatron,          -23,1.5,0       |
    tag: Skene,             7,1,0           | 
    tag: Orchestra,         -9,-1.4,0       | 
    tag: Parodos,           -8.7,-0.2,-23   ;
model_name: greek-theatre-broken | date:10CE;\end{verbatim}
\end{mdframed}
    \caption{An example metadata.txt file for a single site listing the location (latitude, longitude and altitude) followed by the model file names with their dates and tags (with relative positions to the model). }
    \label{fig:metadataexample}
\end{figure}

It is important to note that the URL of the server is unimportant - in a full release of the app the GitHub server could be replaced with a completely bespoke one, but this was outside the scope of this project and opens a set of security issues as to how one is allowed to upload models. A completely accessible system could be open to attack by flooding, for example.

The server information is read into the app from the \verb|ImporterScript| using the \verb|WWW| utility \cite{models:www}, which gets the content from a URL. The metadata is then split up by key characters such as `;' and parsed. All white space is removed from the file so it can be indented as is most useful to the site creator. There is support for different omissions and orderings of the information, and for various date formats. For example, the location and date are technically optional even though there are few cases where a user might choose not to include them. This is achieved by creating a `blank' object to store a site first, and then updating it as new information is found in the folder with the functions \verb|loadObject|, \verb|loadLocation|, \verb|loadDate|, and \verb|loadTag|.

\subsubsection{The Site Hierarchy}
It was not sufficient to simply use the imported .obj file directly as our site. This lead to the design of the main object hierarchy shown in Figure \ref{fig:hierarchydiagram}. All the Sites are contained in a SiteHolder object. Each site has a number of \verb|TimeComponents| which are individually dated and represent it at different periods. A time component has its own model, material and set of tags. 

Each `physical' object in Unity has a transform - a representation of its position, rotation, and size in the scene. When a model (WavefrontObject) is imported, its transform is made a child of the associated time component. Changing the time component will automatically change the model so they can be considered effectively the same object. A similar chaining of transforms is performed from the tags to time components to sites to the site holder, but these are also moved separately.

\begin{figure}[]
    \centering
    \includegraphics[width=1\textwidth]{figures/implementation/importing/objectclassdiagram.png}
        \caption{Simplified class diagram of the Site hierarchy.}
        \label{fig:hierarchydiagram}
\end{figure}

\subsubsection{Tags}
The tags and timelines was planned under the AR Synthesis heading in the Gantt chart, but is detailed here since their implementation required a heavy re-working of the import system and site hierarchy. 

Tags are created by instantiating from a prefab. This is called from the appropriate time component script with the \verb|addTag| function. All tags are added when importing the model and are given their local positions directly from the values in the metadata.txt file.
\newpage
To always be able to read the tag text, they will rotate towards the camera using the function \verb|Quaternion.LookRotation| with the camera transform as input. Note that Unity represents all rotations/orientations using quaternions rather than the standard Euler angles. These are 4-tuples with x, y, z, and w components, which means a series of rotations can efficiently be composed into a single quarternion \cite{models:quaternions}.

Some initial testing for the tags is shown in Figure \ref{fig:ballconetag}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{figures/implementation/importing/ballandconelabelled.jpg}
        \caption{Testing adding one tag each to two simple `sites'.}
        \label{fig:ballconetag}
\end{figure}

\subsubsection{Timeline}
The timeline has two parts to the logic:
\begin{enumerate}
    \item Identify the available dates given the currently loaded sites. Show these and distribute them along the timeline.
    \item For the selected available date $x$ and its successor $y$, display only the time components with a date $x \leq t < y$.
\end{enumerate}

Each site has a sorted set of the dates from its time components. These are passed to the site holder which also checks whether the site should currently be rendered based on its location. If it should, it takes the union of its master set with the site's set of dates and proceeds.

Once all the valid dates are collected by the site holder, they are passed to the script for the timeline (\verb|SliderScript|). As with the tags, the labels on the timeline for each date are instantiated from prefabs and distributed with the lowest available date on the left and the highest on the right. 

The user can make a selection of the current date to view along the timeline and this is stored in the \verb|selectedDate| variable. The selected date is passed back to the site holder which passes it back to the sites. Each site binary searches for either that exact date in its time components, or the nearest component above it, and the corresponding models are rendered. This choice is because it is assumed that a site remains the same over time until a change is explicitly specified by adding another time component. Tests on the timeline are shown in Figure \ref{fig:timelinetest}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/implementation/importing/timelinetest.jpg}
        \caption{A test site at three different arbitrary time periods with their own tags.}
        \label{fig:timelinetest}
\end{figure}

\subsubsection{Fixing the Build}
A serious problem this project faced was that there were no importing issues in the test environment in the Unity editor, but when a full build of the project was made to an APK file, the models were not visible. 

At first it was suspected to be due to the internet connection differences between the testing device and the development device, but following simple tests on reading a text web-page, this was shown to be not the case. Indeed, the models seemed to be importing correctly so it had to be an visual issue. 

With further investigation of OBJ Importer code and documentation, a recommendation to use a particular shader was found: Standard (Specular Setup). In the testing environment all the assets available for development can be used, but since the final build needs to be as small as possible, anything unused will be disregarded. With model importing occurring at runtime, the shader was not recognisably being used at compile time, so was not being included in the final build.

Adding the shader to the explicitly to the final build increases the project memory usage and build-time significantly (by approximately 1GB and 1.5hrs respectively) but the actual application is not as heavily affected and the models render fully.

\subsection{Movement}
\label{movement}

\subsubsection{The Co-Ordinate Systems}
Within the Unity editor, everything has a global position, and child objects also have local positions relative to their parents. These are three dimensional where the x and z axis define the horizontal plane and the y axis defines the height. However, the ARchae application operates on the Geographic Coordinate System (GCS), made up of the latitude and longitude and measured by a mobile device's GPS receiver (GPS co-ordinates). These co-ordinates chart the Earth by measuring the angle in degrees North or South from the equator (latitude) and East or West from the line at Greenwich (longitude) \cite{movement:latitudelongitude}. ARchae also uses altitude - the height above sea-level.

\newpage
For \textbf{disambiguation} purposes, each of the co-ordinate systems will be referred to as such:
\begin{itemize}
    \item GPS: the real measured GCS co-ordinates of the user or site.
    \item Global: where the object appears in the Unity editor using the app co-ordinates.
    \item Local: the Unity co-ordinates using the parent of the object as the origin.
\end{itemize}

There is a class for representation of the GPS co-ordinates, also including altitude. This is the only class developed in this project which does not inherit from the Unity MonoBehaviour class since it is intended purely as a data representation format so does not need updating every frame. 

Latitude and longitude are measured in degrees, and altitude in metres. The values are each stored as a double value. Latitude and longitude are commonly presented in the common form of degrees, minutes and seconds \cite{movement:latitudelongitude}, so there is a \verb|toString| display function that converts the decimal form across. First, the sign is replaced with the corresponding cardinal direction. Then, the decimal part is taken and multiplied by 60 repeatedly for each order of magnitude. This output is what is shown in the UI.

Each site has a GPS location, as does the user. The positioning within the Unity editor was then designed such that one unit of distance is approximately equivalent to one metre in the real world. For a site, this is calculated as the real distance between the site and the user and then scaled by 111139 (approximately one degree in metres). Due to the curvature of the Earth, it is not an exact scalar, and this could be corrected by using the Haversine formula \cite{movement:haversine}. However, on this scale the difference is negligible so the additional computation time is not necessary.

\subsubsection{Updating Positions}
The GPS positions of the sites are loaded upon app start-up, but the globals are updated every frame. The global position of the camera (or user) is fixed and the other objects move relative to it to give the impression that the user is moving through the world. 

This method was chosen over moving the camera itself. To scale the GPS position differences to the global ones correctly, either the user or site's position would need to be translated to the origin. By keeping the user at the origin, this extra step is avoided. 

\subsubsection{Render Distance}
If there are too many vertices in one location, it is possible that the app could begin to struggle to render everything (although it has not so far in testing). To mitigate this, objects within a certain GPS distance of the user are the only ones rendered. Specifically, anything further than $0.01\degree$ ($\approx$ 1km) is hidden.

\subsubsection{Manual Movement}
\label{manualmovement}
Manual movement uses the on-screen arrow buttons and touch control. Pressing the forward key updates the stored GPS locations via the \verb|move| function in the UserLocationScript with the following formula:

\hspace{1cm} $longitude \leftarrow longitude - cos(\theta) \cdot speed \cdot deltaTime$

\hspace{1cm} $latitude  \leftarrow  latitude - sin(\theta) \cdot speed \cdot deltaTime$

where $\theta$ is the angle of the virtual camera about the vertical axis.

The \verb|deltaTime| is a measure of the time between the previous frame and the current. The application is designed to run at 30fps, so the deltaTime will be approximately 1/30, but it is included to account for variation in hardware. Speed by default is set to $5/111139$, meaning the user will travel in manual mode at 5m/s (but this change is made in degrees of latitude and longitude). This speed was selected as it is typical for running \cite{movement:runningspeed}.

For the other buttons, $\theta$ is first changed by multiples of 90\degree, as if the camera had rotated that way and then moved forward.

With regard to rotation of the camera, it was decided to only make this available to the user about the x and y axes, that is they can look up and down, left and right, but not tilt sideways. This simplifies the control since the user's finger on the screen is a movement across a 2D plane and this maps directly onto the two axes of rotation. 

The amount to turn is calculated as the difference between the last recorded touch point and the current one, multiplied by a fixed panning speed, that is:

\hspace{1cm} \textit{offset} $ \leftarrow lastTouch - newTouch$

\hspace{1cm} $eulerRotation \leftarrow panSpeed \cdot ($\textit{offset}$.y, $\textit{offset}$.x, 0)$

The type of touch is also checked for continuity so that a user cannot accidentally move the screen with the palm of their hand, for example \cite{movement:touchcontrol}. 

In addition to the on-screen control, much of the movement functionality was tested using \verb|ContextMenus|. This is a link between the coding environment and the visual editor such that functions marked with the Context Menu attribute appear as clickable buttons in the configuration panel of the corresponding component.

\subsubsection{Live Movement}
The \verb|LIVE| button in the top right of the UI switches between \gls{live} mode. Manual exclusively uses the on-screen controls and live exclusively uses device's GPS to determine the location, and the gyroscope for the orientation. All of the live data collection is done via the \verb|InputHandler2Script|. 

\subsubsection{Mobile Device GPS}
The location is accessed via the Unity location services \cite{movement:locationservice}. There are a number of separate code blocks for initialisation of this service called from the main \verb|StartLocation| function, each handling a common issue with it:

\begin{enumerate}
    \item \textbf{By default, a user often has access permissions to their location disabled.} \\ Location permissions are split into `coarse' and `fine', determining the level of precision allowed. For optimal functionality of the app, fine precision is needed. The functions \verb|coarseLocationPerm| and \verb|fineLocationPerm| check for permissions and then request them as required. This request appears as a pop-up on the user's device when they first open the app.
    \item \textbf{The location service is queried before it is initialised.} \\ The location services has a status flag which is checked before at initialisation and with every \verb|getLocation| request. If it is \verb|Initializing|, it is waited on for a few seconds before being queried again. After 5 unsuccessful attempts to get the location, to avoid unproductive computation, the service will not be queried again.
    \item \textbf{The location service is not given sufficient time to initialise at start-up.} \\ The start-up function returns an \verb|IEnumerator|. This means it runs in its own thread (co-routine) independent of the rest of the code that is refreshed at 30fps. Within reason, the service can take as long as it needs to start and begin outputting live readings.
\end{enumerate}

Every frame the \verb|UserLocationScript| queries the input handler for its last recorded latitude, longitude and altitude, updates its own copy for reference by the sites, and the display on the side bar.

\subsubsection{Gyroscope}
A gyroscope is a ``device containing a rapidly spinning wheel or circulating beam of light that is used to detect the deviation of an object from its desired orientation'' \cite{movement:whatisgyro}. Most modern mobile devices include one so the orientation of the device can be found.

Unity has multiple input systems for interacting with the gyroscope and similar sensors, most importantly the default legacy system and the 2022 newer release version \cite{movement:newinputsystem}. 

The standard solution for accessing orientation information is to query the gyroscope's attitude sensor via the new input system. In addition, as with the GPS service, the sensor needs to be specifically enabled and allocated time to begin collecting data.  

However, in this project the attitude sensor was returning a null output so an appropriate alternative, the \verb|unbiasedRotationRate|, was selected. Where the attitude returns the relative orientation of the device directly, the rotation rate is the change in radians per second. Hence, one can calculate the attitude by continuously updating it by the rotation rate. The `unbiased' part comes from the post-processing applied by Unity to improve the accuracy of the raw hardware output \cite{movement:rotationrate}. Figure \ref{fig:attitudezeros} shows the output of a testing function to display various sensor outputs from the gyroscope in a human-readable format.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/implementation/attitudenoin.PNG}
        \caption{Debug output showing no output from the attitude sensor despite the gyroscope being enabled and available output from the other sensors.}
        \label{fig:attitudezeros}
\end{figure}

Due to instabilities in the new input system, the sensors were returning null in the final build. This is detailed in a bug report which mentions compatibility issues with the primary testing device (Samsung Galaxy Tab S10) \cite{movement:gyroissue}. It also failed on a secondary testing device so in this project the legacy system is used instead. There are no apparent quality differences since there is a smooth mapping from the movement of the device to the virtual camera in the editor. 

\subsection{Augmented Reality Synthesis}
\label{augmentedrealitysynthesis}

\subsubsection{Camera Input}
To turn a VR application into an AR one, camera input needs to be added. This is done via the input handler class. The \verb|startCamera| function searches through available devices for a backward-facing camera and tries to activate it. Failing to find a \verb|back-cam|, it tries to use the first other camera available. A camera pointing towards the user is preferred to no input at all. The ID of the camera is stored and the input is wrapped into a custom material texture. It is important to start the camera only once and store its configuration, since every time it is started the physical shutter is re-opened. The raw input is shown in the corner of Figure \ref{fig:camerabackground}, along with it layered on top of a simulation of the app \cite{arsynthesis:cameratutorial}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/implementation/realbackground.png}
        \caption{The Unity Editor running a simulation of the app taking in the camera input from an external device and creating a custom texture for it.}
        \label{fig:camerabackground}
\end{figure}

\subsubsection{Joining the Real and Virtual Cameras}
The \verb|CameraScript| defines the behaviour of the virtual camera. In manual mode, its rotation is dictated by the store in the \verb|UserLocationScript|, but in live mode, the local processing is a more involved.

There is a blank image component allocated for showing the real camera input on. Using the texture from the input handler function, the \verb|CameraScript| fills this image. 

Since the dimensions of the camera input will not necessarily correspond to the size of the view-port (screen output) some clipping must also occur. The component that all UI elements are contained by is called the `canvas' - it is a digital representation of the screen. By adding a Unity `Aspect Ratio Fitter' to the image component and configuring it to `Envelop Parent`, the image scales uniformly to fill the entire canvas, and possibly overflow over the sides if there is too wide an input. This creates the final seamless display shown in Figure \ref{fig:camerainlab}.

\subsection{Summary}
Models can be imported from the GitHub web-server using the \verb|WWW| utility and OBJ importer. They are then placed into the site hierarchy where one model corresponds to one dated time component, and many time components make up a site. Each time component can be labelled with tags and is available to view by changing the timeline marker.

To move through the virtual world, a user can use the arrow buttons. These find the direction the camera is facing and change the GPS location by approximately 5m (per second) in that direction. Similarly, the user can look around by dragging on the screen. This changes the virtual camera orientation by the difference between the last recorded touch position and the current one. In live mode, a users location is calculated directly from their GPS, and their orientation, from the device movement measured by the gyroscope.

Camera input is taken as a semi-transparent image which is layered in front of the virtual 3D modelled world. This allows the user to see how a site used to be in situ.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/implementation/importing/camerainlab.jpg}
        \caption{Testing the camera in the Department of Computer Science labs with a red icosahedron model.}
        \label{fig:camerainlab}
\end{figure}

\newpage
\section{Testing}

\subsection{Incremental Testing}
As part of the incremental method, full testing occurs at each stage of development to validate that the developer(s) are ready to plan the next increment. For this project, this meant testing the importing and manual movement, then the live movement, and finally the camera, tags and timeline. 

\subsubsection{Increment 1}
Much of the testing for Increment 1 was done in a separate project to the main ARchae development one. This was due to several different experimental approaches being taken to model importing. Table \ref{table:increment1tests} shows a dynamic test plan documenting the cases of model importing once the final strategy showed promise. 

\begin{table}[H]
\footnotesize
\begin{tabular}{llll}
\textbf{\#} & \textbf{Type} & \textbf{Test}                                               & \textbf{Result} \\
1          & Normal        & Imports and displays package sample test case.              & Success.        \\
2          & Normal        & Imports another model from the same server.                 & Success.        \\
3          & Normal        & Imports 3 models from the same server simultaneously.       & Success.        \\
4  & Normal    & Imports simple cube model from GitHub server link.                      & Failed * \\
5          & Normal        & Imports icosahedron from GitHub.                            & Success.        \\
6          & Normal        & Imports 3 models from GitHub simultaneously.                & Success.        \\
7          & Normal        & Reads metadata from GitHub server and outputs to terminal.  & Success.        \\
8          & Normal        & Places 3 models in global co-ordinates defined by metadata. & Success.        \\
9          & Boundary      & Imports only model specified by master list when 3 are uploaded.   & Success         \\
10 & Boundary  & Imports only 3 truly available models when 4 are listed in master file. & Success                                                           \\
11         & Erroneous     & Does not import model of the .fbx type and does not crash.  & Success         \\
12 & Erroneous & Positions model at the origin given invalid metadata co-ordinates.      & Success 
\end{tabular}
\caption{Model importing tests using the OBJ Importer \\ * Amended to the link to the raw data: successful.}
\label{table:increment1tests}
\end{table}

Once success on all tests was achieved, the method was transferred to the ARchae project. 

The other component of this increment was the manual movement. This was tested first using unit tests on the position setting and orientation functions via the Unity Context menu. Then the arrow buttons were added and the two were used in combination to check the virtual GPS location was changing correctly from different starting points.

\subsubsection{Increment 2}
In Increment 1, the app worked only on the development computer, so most of testing in Increment 2 was on specific mobile devices. 

The GPS input was compared to estimates from Google Maps, and tested extensively across the University of Warwick campus and further around England. However, as with most projects featuring specific location information, wider testing is lacking. To confirm generality, it would be beneficial to set up further sites across the UK and beyond, but due to time and monetary restrictions this is not possible within the scope of this project. Some of the tested locations are detailed in the Complete System Testing, but all co-ordinates are for them are listed below by latitude, longitude and altitude:

\begin{multicols}{2}
    \small
    University of Warwick Campus:
    \begin{itemize}
        \item DCS: 52.37531, -1.552912, 142.2
        \item FAB: 52.38073, -1.559563, 154.4
        \item Engineering: 52.381821, -1.562722, 140.8
        \item IBRB: 52.375214, -1.553034, 130.2
        \item Arts Centre: 52.379760, -1.561866, 149.5
        \item Piazza: 52.379015, -1.560941, 138.5;
    \end{itemize}
    
    Wider areas: 
    \begin{itemize}
        \item Coventry Cathedral: 52.407997, -1.507421, 140.1
        \item Kenilworth: 52.347735, -1.591983, 122.7
        \item Cambridge: 52.173594, 0.116592, 20.9 
        \item London: 51.590622, -0.129695, 105.2
        \textcolor{white}{\item} 
        \textcolor{white}{\item} 
    \end{itemize}
\end{multicols}

No serious issues were found at any of these locations. The only note was that sometimes the readings on a phone GPS do not exactly correspond to the documented location on Google Maps, so the set positioning requires adjustments. An example imported model is shown on the test tablet in Figure \ref{fig:testingontablet}.

\begin{figure}[]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/testing/tablet example.jpg}
        \caption{A photo of the testing device having imported an example model.}
        \label{fig:testingontablet}
\end{figure}

Most of the iterative process of working with the sensors is detailed in the implementation (Section \ref{movement}). Specifically, Figure \ref{fig:attitudezeros} shows the feedback from many of the different gyroscopic outputs.

\subsubsection{Increment 3}
Testing for the tags and timeline could be planned similarly to model importing in Increment 1, since they are an extension to it, as shown in Table \ref{table:tags}:
\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{llll}
\textbf{\#} & \textbf{Type} & \textbf{Test}                                                              & \textbf{Result}                       \\
1 & Normal   & Tag information is read from the server and can be outputted to the terminal. & Success.                                       \\
2 & Normal   & A tag can be placed at the centre of a site.                                  & Success (a) \\
3          & Normal        & A tag can be positioned using the global co-ordinate system of the site.   & Success.                              \\
4          & Normal        & Multiple tags can be added likewise.                                       & Success.                              \\
5          & Normal        & Tags always rotate to face the user in the full circuit of the site.       & Success.                              \\
6          & Erroneous     & Tags with erroneous formatting are not included.                           & Success.                              \\
7          & Normal        & Two sites can be added to the server with dates appearing on the timeline. & Success.                              \\
8 & Boundary & One of two sites can have a date excluded.                                    & Success (b)     \\
9          & Normal        & Several sites can be added with a date.                                    & Success                               \\
10         & Boundary      & One site can be added with a date.                                         & Failed (c)                               \\
11         & Erroneous     & Metadata can refer to sites not uploaded.                                  & Success (d) \\
12         & Normal        & Different tags can be ascribed to different time periods.                  & Success                               \\
13         & Normal        & Scrolling the timeline will change the selected period and visible sites.  & Success                               \\
14         & Normal        & Multiple sites can have the same date and be selected together.            & Success                               \\
15         & Boundary      & Selecting a date without a site will display the most recent site.         & Success                              
\end{tabular}
\vspace{0.2cm}
\begin{enumerate}[(a)]
    \item Adjustments to the cosmetics are made.
    \item Note: Dateless sites are always visible.
    \item The app threw an error since there was no way to evenly distribute one site across a range. An extra case was added to cover this.
    \item No site appears, all errors escaped.
\end{enumerate}
\caption{Testing the tags and timeline.}
\label{table:tags}
\end{table}

The camera input itself has been tested on multiple devices, however as with the limitation of locations, there is an inherent limitation on the potential hardware to test on in this project. It would be ideal to access many further mobile devices of different generations to ensure compatibility.

\subsection{Peer Review \& Acceptance Testing}

\subsubsection{Evaluation Day}
Project Evaluation Day was an opportunity to showcase work to peers in its mostly complete form and gather feedback. 

Due to the spatial constrictions of the presentation format it was not suitable to set up a full site showing the building's history. Instead toy example models were collected from online repositories to demonstrate how the app could be used with any model in any arbitrary location. The models used are shown in Figure \ref{fig:evalmodels}. These were tagged and set to reasonable estimations of dates. 

\begin{figure}
    \includegraphics[width=\linewidth]{figures/testing/allnickedmodels.png}
        \caption{Sample models used for evaluation day. In reading order: A Mayan Temple \cite{testing:mayantemple}, a 1970s PC \cite{testing:computer}, an early American locomotive \cite{testing:locomotive}, and a standing stone \cite{testing:standingstone}}.
        \label{fig:evalmodels}
\end{figure}

Three different kinds of feedback were explicitly collected on the day: a poll, a keywords form and a full answer form. The most important one was the poll based on the Technology Acceptance Model, which is a method of determining the success of a new piece of technology \cite{testing:acceptancemodel}. It identifies four factors:
\begin{itemize}
    \item Perceived of Usefulness
    \item Perceived Ease of Use
    \item User Satisfaction
    \item Attribute of Usability
\end{itemize}
These were modified for simplicity and applicability to this project resulting in the four categories:
\begin{itemize}
    \item Usefulness
    \item Intuition
    \item Quality
    \item Customisation Confidence
\end{itemize}
To maximise the amount of responses, participants were asked to rank these categories discretely as shown in Figure \ref{fig:evalscan}, and summarised in the plot Figure \ref{fig:evalplot}. Overall, the feedback was positive, with no Strongly Disagree selected. In particular, respondents felt the application could be incredibly useful. The only area to improve was the `Customisation Confidence'. This pertains to the ability to add one's own models to the app and customise them. Since the demonstrating on evaluation day was mostly of the app itself, this was expected, and the tutorial/demo video created later aims to address this issue.

In addition to this, keyword were gathered that described the project:
\begin{multicols}{2}
\begin{itemize}
    \item Interesting
    \item Cute
    \item Intuitive
    \item Helpful
    \item Clever
    \item Creative
\end{itemize}
\columnbreak
\begin{itemize}
    \item Good Interface
    \item Exciting
    \item Useful
    \item Educational 
    \item Innovative
\end{itemize}
\vfill\null
\end{multicols}
Again, these were all positive and described the aims of this project, especially to the be educational and helpful.

Finally, two long-answer responses were given, as follows:
\begin{itemize}
    \item ``Disagreed on \#3 [Customisation Confidence] due to my technical incompetence! Maybe with a tutorial?''\\
    \textit{This was addressed with the demo video.}
    \item ``Maybe overlay objects over modern ruins, highlighting overlapping edges!'' \\
    \textit{This would be an excellent future work feature requiring computer vision techniques - highlighting the model edges to show exactly where it matches up to the remains as seen through the camera.}
\end{itemize}

\begin{figure}
    \includegraphics[width=0.95\linewidth]{figures/testing/evalscan.pdf}
        \caption{A scan of the primary feedback method from Evaluation Day.}.
        \label{fig:evalscan}
\end{figure}

\begin{figure}
    \includegraphics[width=0.95\linewidth]{figures/testing/evalfeedbackchart.PNG}
        \caption{A graph showing the app review categories against the number of respondents that voted for each level where strongly agree is the best.}.
        \label{fig:evalplot}
\end{figure}

\subsubsection{BCS Lovelace Colloquium}
The BCS Lovelace Colloquium is an annual event geared at women in Computer Science. They run an academic poster competition to which entrants can submit an abstract and later the complete poster on any area of interest in the field. Many finalists present their dissertation projects, and accordingly a poster on ARchae was submitted. 

\textbf{Initial Abstract} \textit{Overcoming the dissonance between a modern day landscape and its historical counterpart can be challenging if it has long gone to ruin. To aid visitors, many popular historical sites provide an immersive series of images showing our best guess of how it would have looked. Sketches on notice boards are commonplace, but we are increasingly seeing virtual or augmented reality experiences as well. In the last year, I have been working on a dissertation project which facilitates exactly this, without creating a bespoke experience from scratch. Users are able to add their own 3D models corresponding to different time periods, label them, and view them in their real-world location. Enabling curators of lesser known historical sites and archaeological researchers, the subject of their interest can be made easily accessible on a mobile device.This poster will detail the benefits of augmented reality in such educational environments and how one could create a virtual experience with this tool. You will see it in action at various points of interest and hear from the public whether they believe learning in a digital environment will ever catch on.}

This was reviewed by two members of the judging panel in order to give pointers for the final poster:\\
\textbf{Reviewer 1}
\textit{AR to support education is a great topic. I like that you set the scene well. Could you add some more detail of what viewers will see? (you mention 3D models but what about textures to add realism? Are the items annotated in some way too?) The last sentence is a bit out of context too: are you doing a user survey? For presentation, this is clearly very visual project so you should be able to get a lot of good images to illustrate your work.}

\textbf{Reviewer 2}
\textit{This is a fascinating poster topic. Have you read the article ``How Virtual Reality is Bringing Historical Sites to Life''? \cite{testing:reviewerpaper} It includes some really good examples of what is currently out there. When designing your poster it would be good to include the definitions of AR and VR and also some images of an historical site as it is and then how it would look in VR or with AR.}

Having taken this feedback into account, a highly visual poster was created using much of the research cited in Section \ref{existingwork} to create the final poster shown in Figure \ref{fig:bcsposter}. 

\begin{figure}[H]
\centering
    \includegraphics[width=0.95\linewidth]{figures/testing/Rowan Mather - Lovelace Poster.pdf}
        \caption{The final submission to the BCS Lovelace Colloquium.}.
        \label{fig:bcsposter}
\end{figure}

\newpage
\subsection{Complete System Testing}

\subsubsection{Ancient Theatre of Epidaurus on the University Piazza}
To put together a complete solution that could be easily tested, a site was selected on campus. The University of Warwick Piazza (Figure \ref{fig:piazza}) has a bowl-shape highly reminiscent of that of an Ancient Greek theatre. Having previously modelled the Ancient Theatre of Epidaurus (Figure \ref{fig:epidaurus}), this was ideal to set up. Although the piazza's history naturally does not date back to the 4th century BCE, it works well as a toy example as shown in Figure \ref{fig:piazzalive}.

\vspace{0.5cm}

\begin{figure}[h]
\begin{minipage}{.48\textwidth}
    \centering
  \includegraphics[width=0.97\linewidth]{figures/testing/epidauruseditor.PNG}
  \caption{3D model based on the Ancient Theatre of Epidaurus.}
  \label{fig:epidaurus}
\end{minipage}
\begin{minipage}{.04\textwidth}
  \hspace{0.1cm}
\end{minipage}
\begin{minipage}{.48\textwidth}
    \centering
  \includegraphics[width=0.75\linewidth]{figures/testing/piazza.jpg}
  \caption{Photo of the University of Warwick Piazza.}
  \label{fig:piazza}
\end{minipage}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=1\linewidth]{figures/testing/archaeonthepiazza.png}
        \caption{Using ARchae on the piazza using the Theatre of Epidaurus model.}
        \label{fig:piazzalive}
\end{figure}

\newpage
As expected, setting up this site required adjustments to the estimated GPS location and scaling. In manual mode, the speed of movement seemed natural and allowed for easy exploration of the parados (side-entrance). In live, as demonstrated in the ARchae demo video, one can look around and walk about the site as required and the perspective will shift in the virtual world accordingly. Two different time periods were tested where their tags labelled the different parts of the Ancient theatre. Tags were perfectly legible - a good size and colour - and were positioned and rotated as expected. Likewise, switching between time periods occurred correctly with no latency.

\subsubsection{Coventry Cathedral}
Coventry Cathedral is a local historic site which was a perfect real-world test case. In 1940 it was heavily bombed by the Luftwaffe which, amongst other damage, completely brought down the roof \cite{testing:covhistory}. Instead of rebuilding the old Cathedral, the ruins were left as a memorial and the New Cathedral was constructed as an attachment. This means that a model of the old cathedral in the app can line up well with some of the remaining walls, and fill in the missing roof. 

The staff at Coventry cathedral were approached for photos and plans of the structure. With the `Blitz Museum' exhibition currently installed in the old crypt, there was sufficient archive material to put together a rough digital replica from scratch. The Blender work-in-progress model using photo references is shown in Figure \ref{fig:covdev}, and the complete demo version is shown in Figure \ref{fig:covfinal}. The ceiling is textured with a semi-transparent, blue material to make the spire visible from the inside, but with the semi-transparency built into the app, this is not necessary. It may be better to texture the walls that line up as such and the missing parts normally. 

Having spoken to one of the curators, Adam, at the site he explained that often the Red Hollington sandstone of the cathedral can tamper with the GPS signal, so for general use it will require further adjustments to ensure accuracy. Another known issue is `Urban Canyons': areas with narrow streets and tall buildings where the GPS signal can be obscured \cite{testing:urbancanyon}. The area around the cathedral is densely packed so it is likely that this effect is quite strong.

\begin{figure}[h]
\centering
    \includegraphics[width=1\linewidth]{figures/testing/covcathedraldev.png}
        \caption{Modelling Coventry Cathedral in Blender.}
        \label{fig:covdev}
\end{figure}

\begin{figure}[H]
\centering
    \includegraphics[width=0.7\linewidth]{figures/testing/covcathedralfinal.PNG}
        \caption{The demo model of the Coventry Cathedral  which was used in the app.}
        \label{fig:covfinal}
\end{figure}

Nevertheless, Adam was impressed with the test version saying, ``Once it’s passed its beta testing, please bring this back. It’s something we would really love to use.''

In particular, the view of the nave was effective in the illusion of reconstruction, completing the pillars that are just bases in the modern day. Figures \ref{fig:cath1} and \ref{fig:cath2} show a peer exploring the ruins using the app.

It would be ideal to use the Coventry Cathedral as the first generally available site with ARchae, and this is the intention after the assessment of this project is complete.

\begin{figure}[H]
\centering
    \includegraphics[width=0.43\linewidth]{figures/testing/covtest1.jpg}
        \caption{ARchae being used in the aisle of the Coventry Cathedral ruins.}
        \label{fig:cath1}
\end{figure}

\begin{figure}[H]
\centering
    \includegraphics[width=0.43\linewidth]{figures/testing/covtest2.jpg}
        \caption{ARchae being used outside Coventry Cathedral, looking through the windows.}
        \label{fig:cath2}
\end{figure}

\newpage
\subsection{Stress Testing}
To ensure the system was sufficiently robust, some sample sites were uploaded that tested extreme cases.

\subsubsection{One Complex Model}
Firstly, it was critical to check that the system could handle a large number of vertices. Often in order to precisely map a site, photogrammetric software will output a highly complex model with many points. To simulate this, a plane was generated and subdivided many times to create thousands of squares. Then the positions of the vertices were randomised along their normal (the vertical z-axis in this case). Figure \ref{fig:stressplane} shows the plane with 10,404 points.

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/testing/manyvertices.PNG}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.835\linewidth]{figures/testing/manyverticesrandomised.PNG}
\end{minipage}
\caption{The initial plane (created in Blender) used for stress-testing, subdivided to include 10,404 vertices. The right image shows the vertices at randomised heights.}
\label{fig:stressplane}
\end{figure}

The system handled this without fault, loading the model instantly (see Figure \ref{fig:manyverticesapp}). There was no latency whilst moving around in manual or live mode, or any visual artefacts (anomalies). 

\subsubsection{Many Models}
Similarly, a site may be constructed of many models, each representing a building for example. It is common when modelling a city to use many primitive objects, such as cuboids and applying the detailing exclusively as textures. To simulate this 25 cubes were generated. 17 are placed in a line and the rest are scattered arbitrarily (see figure \ref{fig:manymodelsapp}). When initially booting the app, there is some loading time - approximately 10 seconds, but as with the single complex model, the app handles rendering all models in manual and live mode without any strain. 

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/testing/manyverticesapp.jpg}
    \caption{Stress test on one complex model.}
  \label{fig:manyverticesapp}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/testing/manymodelsapp2.jpg}
  \caption{Test on many models in the app.}
    \label{fig:manymodelsapp}
\end{minipage}
\end{figure}

\subsection{Summary}
In summary, the incremental method of testing was effective in catching errors at every stage. Having a Minimum Viable Product built correctly early on made it much easier to iterate development, since the system could be reverted to a robust version. 

Similarly, having two complete versions also validated the correctness of the app, demonstrating that it could be used for its intended audience, which would be critical for pitching the app to curators.

Expanding use of the app is proven to be possible on both the technical level and social level. The stress tests demonstrate that many sites can be added without the system being overloaded, and the user acceptance testing show that people are interested by the possibilities of the technology and are willing to try it further.

\newpage
\section{Evaluation}
\label{Evaluation}
Overall, this project has been successful. Everything required for the the minimum viable product has been completed, along with many additional features. A full toy example is operational on the piazza, along with a solution at Coventry Cathedral. 

ARchae has proven to be a robust system, running with minimal latency other than some initial boot time. Feedback from Evaluation Day, the Lovelace Colloquium and the Coventry Cathedral staff regarding both the subject area and the implementation itself has been overwhelmingly positive.

The main limitations to the system derive from a lack of resources. It would be informative to test the application in many more places and on different devices, and to make it available to download from an app store. In addition, distribution of the ARchae system would be easier with a bespoke upload point for models.

In terms of project management, using the incremental method and continuously referring to the Gantt chart has enforced a good distribution of workload. Term 1 was heavier on development as the basic system was constructed, leaving more room for experimentation and testing in Term 2.

\subsection{Objective Summary}
The objectives have been well covered with over half of the extension work tackled, as shown in Table \ref{table:objectivesummary}. Many of the incomplete extensions were initial conceptions which have proved unnecessary with further research and some of the objectives have been completed out of prioritisation order. However, they have remained unchanged - all of them could have been done with sufficient time.  

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
Minimum Viable Product (Must)          & 100\% \\ \hline
Additional (Should) & 92\%  \\ \hline
Extension (Could)   & 62\%  \\ \hline
\end{tabular}
\caption{Objective priority against completeness.}
\label{table:objectivesummary}
\end{table}

\subsection{Objective Full Review}
Tables \ref{table:obj1} to \ref{table:obj5} re-list the objectives, with those in \done{green} being the completed ones. The reason for omission or method of completion are detailed.

The $\uparrow$ denotes the priority: Must-Have, Should-Have, Could-Have.

\begin{table}[H]
\caption{\textbf{I. User may upload a model to a remote server.}}

\begin{tabular}{| p{0.02\linewidth} | p{0.02\linewidth} | p{0.4\linewidth} | p{0.4\linewidth} | }
\hline
\textbf{\#} & \textbf{$\uparrow$} & \textbf{Objective} & \textbf{Response}
\\ \hline
1 & M & \done{Models will be in the wavefront (.obj) form. } & The importing method requires this. \\ \hline
2 & C & Models can be uploaded in additional common formats e.g. .fbx, .dae, .blend & The package used worked specifically .obj types. This would be a simple but time-consuming job to expand, and most models can easily be converted to .obj. \\ \hline
3 & M & \done{User may upload a texture and other metadata to pair with the model.} & A .mtl file can also be placed in the repository and this is used by the object importer in conjunction. All other information is in the metadata file. \\ \hline
4 & S & \done{A subset of the server models can be externally selected to be available in app.} & The site list file in the repository contains the names of all sites to be imported. \\ \hline
5 & C & \done{Users cannot upload without submitting a review request.} & This would need to be modified for a different server set-up, but as such one can use the standard git functionality such as pull-requests to make reviewed changes to the server. \\ \hline
\end{tabular}
\label{table:obj1}
\end{table}

\begin{table}[H]
\caption{\textbf{II. User may download models from the server into their local app.}}
\begin{tabular}{| p{0.02\linewidth} | p{0.02\linewidth} | p{0.4\linewidth} | p{0.4\linewidth} | }
\hline
\textbf{\#} & \textbf{$\uparrow$} & \textbf{Objective} & \textbf{Response}
\\
\hline
1 & M & \done{Models can be imported from the server in their pure object form. } & Model files are imported and wrapped into the site hierarchy. \\ \hline
2 & S & \done{They can be coloured/textured according to an accompanying texture file (.mtl).} & The OBJ Importer makes use of the mtl file. \\ \hline
3 & S &  Users can select a subset of the server models to import locally. & This objective was planned for term 3 but was not reached due to presentation work. It should not be difficult to add a checklist as designed. \\ \hline
4 & S & \done{Models will render with their textures and metadata.} & In all tests so far models have rendered correctly. \\ \hline
5 & M & \done{Models will render with reasonable latency, generally a maximum of 2 seconds per model.} & In all tests so far models render within less than a second, although the app itself can take up to 10 seconds to load. \\ \hline
\end{tabular}
\label{table:obj2}
\end{table}

\newpage
Table 6: \textbf{III. User will be able to set their location and move around the models.}
\begin{longtable}{| p{0.02\linewidth} | p{0.02\linewidth} | p{0.4\linewidth} | p{0.4\linewidth} | }
\hline
\textbf{\#} & \textbf{$\uparrow$} & \textbf{Objective} & \textbf{Response}
\\ \hline
1 & M & \done{User has a manually configurable geo-location (latitude \& longitude).} & Users can use the arrow buttons to explore the virtual world in manual mode. \\ \hline
2 & M & \done{User has a manually configurable orientation.} & Users can touch the screen and drag to look around in manual mode. \\ \hline
3 & M & \done{Models can be placed in a geo-location relative to the user.} & The virtual position of a site is calculated by the difference between its GPS position and the user's.\\ \hline
4 & S & \done{Models will render only when within a certain distance to the user.} & Models further away than 0.01 degrees of latitude/longitude are not displayed. \\ \hline
5 & S & User can change the render distance. & This was determined to be unnecessary and would make the UI design overly complex. There should be very few use-cases where a user needs to see a model further than 1km away. \\ \hline
6 & C & User can change the scaling of the whole scene. & This does not make sense as a feature in live mode. It would be simple but time-consuming to implement in manual mode and would further clutter the interface. \\ \hline
7 & S & \done{User location can be set to their actual mobile device's geo-location.} & Accessed via the Unity input system, the virtual location is set to the GPS co-ordinates and altitude. \\ \hline
8 & S & \done{User can move freely while the app tracks their geo-location.} & The GPS location is queried every frame at 30fps. \\ \hline
9 & S & \done{Compass direction can be set to that of device.} & The compass is not used explicitly, but the gyroscope which determines the real change in rotation of the device. \\ \hline
10 & S & \done{Tilt can be set to the spirit level of device.} & As above. \\ \hline
11 & S & \done{User can switch between manual and device modes.} & This is manual and live mode which are toggleable via the button in the top right of the UI.  \\ \hline
12 & S & \done{Add camera input as background for the scene.} & The camera input is placed as a custom texture onto a plane in the UI. It is not strictly a background, more a foreground, but appears the same. \\ \hline
13 & C & Have a slider for model transparency. & The models can be hard to see as it is with the live input, so this was deemed unnecessary. They are always fully visible. \\ \hline
\captionsetup{labelfont={color=white}}
\caption{}
\label{table:obj3}
\end{longtable}

\newpage
\begin{table}[H]
\caption{\textbf{IV. Additional information about the sites will be accessible.}}
\begin{tabular}{| p{0.02\linewidth} | p{0.02\linewidth} | p{0.4\linewidth} | p{0.4\linewidth} | }
\hline
\textbf{\#} & \textbf{$\uparrow$} & \textbf{Objective} & \textbf{Response}
\\ \hline
1 & C & \done{Models can be tagged with information in a specific relative location to the model.} & Tags placed in the metadata file with their relative location will display on the model and rotate to face the user. \\ \hline
2 & C & Tags can be clicked to view further information/photos. & It was decided to keep the UI simple so the tags are more for labelling components than detailing full histories. \\ \hline
3 & C & Tags can be shown/hidden in scene. & Since the tags are typically single phrases (as above) it is not necessary to remove them. \\ \hline
4 & C & \done{A date (range) will be in the corner corresponding to the age of the site you are viewing.} & This is specified by the timeline. \\ \hline
5 & C & \done{A timeline scroll will be at the bottom of the screen.} & The timeline is available with a pointer. \\ \hline
6 & C & \done{Models can be attached to a specific time period and will only be visible when this is selected.} & The site holder object has a master set of all available times and the timeline script feeds back to it which one is currently selected. All models associated with a different time are hidden. \\ \hline
\end{tabular}
\label{table:obj4}
\end{table}

\begin{table}[H]
\caption{\textbf{V. The app will be intuitive and guided.}}
\begin{tabular}{| p{0.02\linewidth} | p{0.02\linewidth} | p{0.4\linewidth} | p{0.4\linewidth} | }
\hline
\textbf{\#} & \textbf{$\uparrow$} & \textbf{Objective} & \textbf{Response}
\\ \hline
1 & C & \done{Include a menu with an app tutorial.} & There both a main menu with a help screen and a video tutorial available online. \\ \hline
2 & C & \done{Most information can be hidden so the user is not overloaded.} & The side-bar with additional information can be slid away, and in live mode all manual functionality to avoid confusion. \\ \hline
3 & C & \done{The default view will be a simple drop-down list of available models and device mode.} & The available models are listed in the side bar and by default the app is in manual mode. Switching into live mode first would be unwise since it can be harder to navigate. \\ \hline
\end{tabular}
\label{table:obj5}
\end{table}


\newpage
\section{Future Work}
\label{futurework}
Throughout this report, smaller points of out-of-scope work have been detailed. Addressed below are major features which were not planned for this project and would require much more time than allowed, but would be beneficial for the app and interesting to consider.

\subsection{Lightmapping}
Lightmapping is a potential optimisation in the rendering of the sites. It involves `baking' the lighting state into the texture as shown in Figure \ref{fig:lightmapping}. Since the lighting on the models is always the same - the sun-like projection - it does not need to be continuously recomputed. By lightmapping the models, the computation is performed only once.

\begin{figure}[h]
\centering
    \includegraphics[width=0.9\linewidth]{figures/futurework/Lightmap.png}
        \caption{Lightmapping a cube. The right-hand image shows the flattened texture including the lighting state. \cite{futurework:lightmapping}}
        \label{fig:lightmapping}
\end{figure}

\subsection{Level-of-Detail}
The level-of-detail pertains to how many vertices are currently being used to display a 3D model (Figure \ref{fig:levelofdetail}). In this project, the complete set of vertices is always used, however it could be optimal to reduce the number as the model gets further away from the user.

\begin{figure}
\centering
    \includegraphics[width=1\linewidth]{figures/futurework/levelofdetail.png}
        \caption{A model at two different levels of detail. More vertices are included in the left-hand side. \cite{futurework:levelofdetail}}
        \label{fig:levelofdetail}
\end{figure}

Just as with model importing (see Section \ref{modelimporting}) this is a well-documented and fairly trivial process if the levels can be pre-computed in the 3D modelling software and imported into the project at compile time \cite{futurework:levelofdetail} \cite{futurework:LODbrackeys}. The difficulty arises from performing this at runtime since one has to compute the levels in Unity with no prior knowledge about the model.

In researching this feature, an asset was found called NanoLOD \cite{futurework:nanoLOD}. Whilst still designed to work at compile time, this asset computes a \verb|LOD Group| in Unity automatically for a model, so with some adjustment it could be a viable strategy to attempt this. 

\subsection{Bluetooth Waypoints}
The best accuracy of GPS is achieved with two receivers, but this is typically only used in the military \cite{futurework:gpsaccuracy}. In the worst cases of some highly built-up areas or underground use it can be up to around 10m out. In general, this is not a serious issue, but to improve the accuracy of a mobile device's location, sites could be able to add optional Bluetooth beacons. 

Bluetooth beacons are low-energy devices that can be used to determine proximity, acting as way-points. With multiple beacons placed around a site, the location of the user (to about 1m accuracy \cite{futurework:beaconaccuracy}) can be calculated using trilateration, that is using the distance from three known points to find the position of a fourth.

Since the goal of this project is to be a generalist application, and there is more specific (and expensive) technology necessary to set-up Bluetooth beacons, creating a site should never require them. There could just be compatibility to include them as a feature.

\subsection{Time of Day and Weather}
This expansion was from final peer feedback. They suggested adding the option to change the weather and time of day in the manual mode, purely for aesthetic purposes. This would add an additional level of alignment with the real world - and it could even be possible to match the time of day in live mode to the device's clock. It would likely not pose too great of a technical challenge, but would be time consuming to implement.

Unity has a number of assets to assist with creating natural environments and simulating weather. In particular, the one created by Tobias Johansson could be suitable for this purpose \cite{futurework:weather} (sample project shown in Figure \ref{fig:weather}).

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/futurework/tobysunset.PNG}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.96\linewidth]{figures/futurework/tobynight.PNG}
\end{minipage}
\caption{Unity asset for creating different times of day 
and weather environments \cite{futurework:weather}.}
\label{fig:weather}
\end{figure}

\newpage
\section{Conclusion}
In this project, an application for displaying historical sites in augmented reality has been developed. Via the upload point, a curator can set up a site with various 3D models and make it available for visitors to walk around with a mobile device, using their GPS to locate the site. 

The interface was designed from scratch and implemented using the Unity game engine, and all of the web access and location processing was written in C\# in the Unity project. 

Models are displayed with texturing detail from the associated .mtl file and render with minimal latency even under high load. 

Sites can be made visible at different time periods and each period can be labelled with tags to identify key features and highlight the changes in the structure over time.

Going forward, the success of this project will be evident from its use. It is the intention to finalise the model at Coventry Cathedral and make the software publicly available to visitors to explore.

Until it is placed on the app store, ARchae can be installed and run on mobile devices by following the instructions given in \textbf{Appendix \ref{howtorun}}.

\newpage
\subsection*{\centering Acknowledgments}
This author would like to acknowledge my supervisor Dr Claire Rocks for you ongoing support, guidance and general loveliness.

To my parents for their extensive historical knowledge and early insights as to useful features of this project, and for proof reading this report.

Also to Rosella Suma for her combination of technical and archaeological guidance.

To the staff at Coventry Cathedral for their support, input, and provision of research material.

And to Alex and Maddie, and my wonderful housemates Andrew, Luca, Ruby, Ryan, and Saskia for being my test subjects, ducks, and immaculate proof-readers.

And finally thank you to my partner Bryce for keeping me sane and righting me again when absolutely nothing worked. 

\newpage
\bibliography{bibliography}

\newpage
\appendix

\section{How to Run ARchae}
\label{howtorun}
Since the app is not yet publicly available in an app store, the steps to compile and run the software on an Android device, and those to add your own site are given below. For a future user it will simply be downloadable.

\subsection{Installing with the APK}
If you have the ARchae .apk file then you can follow the standard procedure for manually installing an App on an android device:

\begin{enumerate}
    \item Download the app to a PC and connect it to your device.
    \item Place it into the root folder of your device.
    \item Navigate to it in your file manager and click to install.
\end{enumerate}

\subsection{Installing from the Project Files}

\subsubsection{Setting up Unity}
\begin{enumerate}
    \item You will need a Unity installation. Development was done in Unity 2022.3.9, so that is the recommended version.
    \item You can import the project attached to this report, or download it from the GitHub.
    \item You will then need the Android SDK development toolkit. This is optionally installable when you first open Unity, or go to File $\succ\succ$ Build Settings $\succ\succ$ Android and you can install from there.
\end{enumerate}

\subsubsection{Unity Remote}
\begin{enumerate}
    \item On your mobile device, install the Unity Remote app.
    \item Plug your device into the development machine and ensure all debug permissions are granted.
    \item Press play at the top of the Unity editor and you should see the app appear on your device screen. This is the testing environment.
    \item To place a version permanently onto the device, you will need to build it: File $\succ\succ$ Build Settings $\succ\succ$ Android $\succ\succ$ Build and Run.
\end{enumerate}

\subsection{Adding a Site}
\begin{enumerate}
    \item This is best demonstrated via the video demo.
    \item First go to the GitHub page for sites. And request developer permissions.
    \item Create a new folder for your site and add the name of the folder to the object list text file.
    \item Drag any object and material files for your site into the folder.
    \item Add a metadata.txt file for the location, tagging, and timeline information as detailed above.
\end{enumerate}

\end{document}
